{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses a DenseNet model to detect abnormal cells from thin blood smear images. The DenseNet model is forked from https://github.com/titu1994/DenseNet. The code can be modified to use several configurations of DenseNet. The weights can be initialized to \"None\" to custom train on the dataset or 'imagenet' to intialize with the pre-trained ImageNet weights and then train the model end-to-end. The model has been modified by adding a global average pooling (GAP) layer and a final dense layer to the truncated model. You shall optimize the model hyperparameters to suit your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, let us define a few functions to load the data and convert them to Keras compatible targets. We will load the libraries to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import densenet\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed 5-fold cross validation at the patient level. we had train and test splits for each fold to ensure that none of the patienet information in the training data leaks into the test data. We randomly split 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define data directories\n",
    "train_data_dir = 'f1_mal/train' #path to your data\n",
    "valid_data_dir = 'f1_mal/valid'\n",
    "test_data_dir = 'f1_mal/test'\n",
    "\n",
    "# declare the number of samples in each category\n",
    "nb_train_samples = 19808 #  modify for your dataset\n",
    "nb_valid_samples = 4952 #  modify for your dataset\n",
    "nb_test_samples = 2730 # modify for your dataset\n",
    "num_classes = 2 # binary classification \n",
    "img_rows_orig = 100 # modify these values depending on your requirements\n",
    "img_cols_orig = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define functions to load and resize the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    labels = os.listdir(train_data_dir)\n",
    "    total = len(labels)\n",
    "    X_train = np.ndarray((nb_train_samples, img_rows_orig, img_cols_orig, 3), dtype=np.uint8)\n",
    "    Y_train = np.zeros((nb_train_samples,), dtype='uint8')\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    j = 0\n",
    "    for label in labels:\n",
    "        image_names_train = os.listdir(os.path.join(train_data_dir, label))\n",
    "        total = len(image_names_train)\n",
    "        print(label, total)\n",
    "        for image_name in image_names_train:\n",
    "            img = cv2.imread(os.path.join(train_data_dir, label, image_name), cv2.IMREAD_COLOR)\n",
    "            img = np.array([img])\n",
    "            X_train[i] = img\n",
    "            Y_train[i] = j\n",
    "            if i % 100 == 0:\n",
    "                print('Done: {0}/{1} images'.format(i, total))\n",
    "            i += 1\n",
    "        j += 1    \n",
    "    print(i)                \n",
    "    print('Loading done.')\n",
    "    print('Transform targets to keras compatible format.')\n",
    "    Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
    "    np.save('imgs_train.npy', X_train, Y_train) #save as numpy files\n",
    "    return X_train, Y_train\n",
    "    \n",
    "def load_validation_data():\n",
    "    # Load validation images\n",
    "    labels = os.listdir(valid_data_dir)\n",
    "    X_valid = np.ndarray((nb_valid_samples, img_rows_orig, img_cols_orig, 3), dtype=np.uint8)\n",
    "    Y_valid = np.zeros((nb_valid_samples,), dtype='uint8')\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating validation images...')\n",
    "    print('-'*30)\n",
    "    j = 0\n",
    "    for label in labels:\n",
    "        image_names_valid = os.listdir(os.path.join(valid_data_dir, label))\n",
    "        total = len(image_names_valid)\n",
    "        print(label, total)\n",
    "        for image_name in image_names_valid:\n",
    "            img = cv2.imread(os.path.join(valid_data_dir, label, image_name), cv2.IMREAD_COLOR)\n",
    "            img = np.array([img])\n",
    "            X_valid[i] = img\n",
    "            Y_valid[i] = j\n",
    "            if i % 100 == 0:\n",
    "                print('Done: {0}/{1} images'.format(i, total))\n",
    "            i += 1\n",
    "        j += 1\n",
    "    print(i)            \n",
    "    print('Loading done.')\n",
    "    print('Transform targets to keras compatible format.');\n",
    "    Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
    "    np.save('imgs_valid.npy', X_valid, Y_valid) #save as numpy files\n",
    "    return X_valid, Y_valid\n",
    "\n",
    "def load_test_data():\n",
    "    labels = os.listdir(test_data_dir)\n",
    "    X_test = np.ndarray((nb_test_samples, img_rows_orig, img_cols_orig, 3), dtype=np.uint8)\n",
    "    Y_test = np.zeros((nb_test_samples,), dtype='uint8')\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    j = 0\n",
    "    for label in labels:\n",
    "        image_names_test = os.listdir(os.path.join(test_data_dir, label))\n",
    "        total = len(image_names_test)\n",
    "        print(label, total)\n",
    "        for image_name in image_names_test:\n",
    "            img = cv2.imread(os.path.join(test_data_dir, label, image_name), cv2.IMREAD_COLOR)\n",
    "            img = np.array([img])\n",
    "            X_test[i] = img\n",
    "            Y_test[i] = j\n",
    "            if i % 100 == 0:\n",
    "                print('Done: {0}/{1} images'.format(i, total))\n",
    "            i += 1\n",
    "        j += 1\n",
    "    print(i)            \n",
    "    print('Loading done.')\n",
    "    print('Transform targets to keras compatible format.');\n",
    "    Y_test = np_utils.to_categorical(Y_test[:nb_test_samples], num_classes)\n",
    "    np.save('imgs_test.npy', X_test, Y_test) #save as numpy files\n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define functions to resize the original images to that dimensions required for the pretrained models using the functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_resized_training_data(img_rows, img_cols):\n",
    "\n",
    "    X_train, Y_train = load_training_data()\n",
    "    X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
    "    \n",
    "    return X_train, Y_train\n",
    "    \n",
    "def load_resized_validation_data(img_rows, img_cols):\n",
    "\n",
    "    X_valid, Y_valid = load_validation_data()\n",
    "    X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
    "        \n",
    "    return X_valid, Y_valid   \n",
    "\n",
    "def load_resized_test_data(img_rows, img_cols):\n",
    "\n",
    "    X_test, Y_test = load_test_data()\n",
    "    X_test = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_test[:nb_test_samples,:,:,:]])\n",
    "    \n",
    "    return X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An evaluation script has been written to compute the confusion matrix for the performance of the trained model. This function prints and plots the confusion matrix. Normalization can be applied by setting 'normalize=True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False, #if true all values in confusion matrix is between 0 and 1\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to train a DenseNet model with depth 40, with the presence of bottleneck layers and initialized with imagenet weights. The model is added with a final dense layer to predict on the normal/abnormal cell categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with K.tf.device('/gpu:0'):\n",
    "        img_rows=100\n",
    "        img_cols=100\n",
    "        channel = 3\n",
    "        num_classes = 2 \n",
    "        batch_size = 8 \n",
    "        nb_epoch = 60\n",
    "   \n",
    "        #declare a weight function to store the weights\n",
    "        model_final_weights_fn = 'custom_densenet_malaria.h5'\n",
    "   \n",
    "        # Load our model\n",
    "        base_model = densenet.DenseNet(input_shape=(100,100,3), include_top=False, \n",
    "                                       classes=num_classes, depth=40, nb_dense_block=4, growth_rate=12,\n",
    "                                       bottleneck=True, reduction=0.5, weight_decay=1e-4, \n",
    "                                       weights='imagenet', activation='softmax')\n",
    "            \n",
    "        base_model.summary()\n",
    "        \n",
    "        #add a new dense layer to predict on binary classes        \n",
    "        x = base_model.output\n",
    "        predictions = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "        # this is the model we will train\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        model.summary()\n",
    "\n",
    "        # Load data\n",
    "        X_train, Y_train = load_resized_training_data(img_rows, img_cols)\n",
    "        X_valid, Y_valid = load_resized_validation_data(img_rows, img_cols)\n",
    "        \n",
    "        # print data shape\n",
    "        print(X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape)\n",
    "\n",
    "        # Start Fine-tuning\n",
    "        sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True) #try varying this for your task and see the best fit\n",
    "        model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "        print('-'*30)\n",
    "        print('Start Training the DenseNet on the Malaria Cell Dataset...')\n",
    "        print('-'*30)\n",
    "        \n",
    "        #compute training time\n",
    "        t=time.time()\n",
    "        \n",
    "        #start model training\n",
    "        hist=model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "              shuffle=True,\n",
    "              verbose=1,\n",
    "              validation_data=(X_valid, Y_valid),\n",
    "              )\n",
    "        print('Training time: %s' % (time.time()-t))\n",
    "        \n",
    "        #plot performance graphs\n",
    "        train_loss=hist.history['loss']\n",
    "        val_loss=hist.history['val_loss']\n",
    "        train_acc=hist.history['acc']\n",
    "        val_acc=hist.history['val_acc']\n",
    "        xc=range(nb_epoch)\n",
    "        \n",
    "        plt.figure(1,figsize=(20,10), dpi=100)\n",
    "        plt.plot(xc,train_loss)\n",
    "        plt.plot(xc,val_loss)\n",
    "        plt.xlabel('num of Epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('train_loss vs val_loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend(['train','val'])\n",
    "        plt.style.use(['classic'])\n",
    "        \n",
    "        plt.figure(2,figsize=(20,10), dpi=100)\n",
    "        plt.plot(xc,train_acc)\n",
    "        plt.plot(xc,val_acc)\n",
    "        plt.xlabel('num of Epochs')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.title('train_acc vs val_acc')\n",
    "        plt.grid(True)\n",
    "        plt.legend(['train','val'])\n",
    "        plt.style.use(['classic'])\n",
    "        \n",
    "        #save the model weights         \n",
    "        model.save_weights(model_final_weights_fn)\n",
    "        \n",
    "        # save architecture and weights together if you prefer\n",
    "        model.save('custom_densenet_malaria_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make predictions with the trained DenseNet-121 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with K.tf.device('/gpu:0'):\n",
    "\n",
    "        model_final_weights_fn = 'custom_densenet_malaria.h5'\n",
    "        model.load_weights(model_final_weights_fn, by_name=True) \n",
    "    \n",
    "        #load test data\n",
    "        X_test, Y_test = load_resized_test_data(img_rows, img_cols)\n",
    "        print(X_test.shape, Y_test.shape)\n",
    "    \n",
    "        # Make predictions\n",
    "        print('-'*30)\n",
    "        print('Predicting on test data...')\n",
    "        print('-'*30)\n",
    "        Y_test_pred = model.predict(X_test, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "        #compute the ROC-AUC values\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(num_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], Y_test_pred[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), Y_test_pred.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        plt.figure(figsize=(20,10), dpi=100)\n",
    "        lw = 1\n",
    "        plt.plot(fpr[1], tpr[1], color='red',\n",
    "                 lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[1])\n",
    "        plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristics')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # compute the test accuracy\n",
    "        Test_accuracy = accuracy_score(Y_test.argmax(axis=-1),Y_test_pred.argmax(axis=-1))\n",
    "        print(\"Test_Accuracy of DenseNet Model is: = \",Test_accuracy)\n",
    "    \n",
    "         # Cross-entropy loss score\n",
    "        score = log_loss(Y_test, Y_test_pred)\n",
    "        print(score)\n",
    "        \n",
    "        # Average precision score\n",
    "        prec_score = average_precision_score(Y_test,Y_test_pred)  \n",
    "        print(prec_score)\n",
    "    \n",
    "        # transfer it back and save the predictions\n",
    "        Y_test_pred = np.argmax(Y_test_pred, axis=1)\n",
    "        Y_test = np.argmax(Y_test, axis=1)\n",
    "        print(Y_test_pred)\n",
    "        print(Y_test)\n",
    "        np.savetxt('malaria_Y_test_pred.csv',Y_test_pred,fmt='%i',delimiter = \",\")\n",
    "        np.savetxt('malaria_Y_test.csv',Y_test,fmt='%i',delimiter = \",\")\n",
    "    \n",
    "        #plot confusion matrix\n",
    "        target_names = ['class 0(parasitic)', 'class 1(normal)']\n",
    "        print(classification_report(Y_test,Y_test_pred,target_names=target_names))\n",
    "        print(confusion_matrix(Y_test,Y_test_pred))\n",
    "        cnf_matrix = (confusion_matrix(Y_test,Y_test_pred))\n",
    "        np.set_printoptions(precision=4)\n",
    "        plt.figure(figsize=(20,10), dpi=100)\n",
    "        plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                          title='Confusion matrix')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
